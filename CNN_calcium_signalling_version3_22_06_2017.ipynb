{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import keras\n",
    "from keras.layers import convolutional, Dense, Activation,pooling\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Flatten, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import cPickle as pickle\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################### LOADING DATA ##############################\n",
    "\n",
    "#USER INTERFACE \n",
    "\n",
    "### Create a dictionary that contains all the dictiories within each dataset (.mat file) (workspace) we want to load ###\n",
    "#SELECT data directory\n",
    "dataDir = input(\"Enter data directory path: \")  #dataDir = \"/scratch/barbieri/DATA_CNN_networks/Data/\"\n",
    "# \"/scratch/barbieri/DATA_CNN_networks/data_pickle/\"\n",
    "\n",
    "\n",
    "#Load data\n",
    "Load_Data = {}  \n",
    "for i in range(0, len(os.listdir( dataDir ))):  \n",
    "    Load_Data[i] = {}  #Dictionary for a single workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN_parameters():\n",
    "    # Please edit parameters\n",
    "    \n",
    "    ## =========================================================================\n",
    "    # Parameters for the CNN\n",
    "    \n",
    "    # Define dictionary for the parameters\n",
    "    PARAMETERS = {}\n",
    "    \n",
    "    # Number of classes\n",
    "    PARAMETERS['n_classes'] = 6  #3 or 6\n",
    "    \n",
    "    # Choose Dropout value\n",
    "    PARAMETERS['dropout'] = 0.4\n",
    "    \n",
    "    # Define number of epochs\n",
    "    PARAMETERS['n_epoch'] = 4 #800\n",
    "         \n",
    "    # Define batch size\n",
    "    PARAMETERS['batch_size'] = 1 #50\n",
    "            \n",
    "    # Define decay value to be tested: write in array format\n",
    "    PARAMETERS['decay_value'] = [0.0001]\n",
    "    \n",
    "    # Define learning rate\n",
    "    PARAMETERS['learning_rate'] = 0.001\n",
    "    \n",
    "    # Define the number of iteration per CNN\n",
    "    PARAMETERS['n_iteration'] = 2 #5\n",
    "            \n",
    "    ## =========================================================================\n",
    "    # Parameters for the data\n",
    "            \n",
    "    # Choose the number of curves to use as training data\n",
    "    PARAMETERS['n_training_data'] = 1800\n",
    "\n",
    "    # Choose number of point to downsample an intensity curve\n",
    "    PARAMETERS['points_curve'] = 128\n",
    "    \n",
    "    return PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_dict(Load_Data):\n",
    "    \n",
    "    #### Define another dictionary for storing intensity curves and classes ### \n",
    "    #From each dataset, the intensity curve and classes are extracted and stored in another dictionary\n",
    "    DATA = {}\n",
    "    N_of_file =len(os.listdir( dataDir ))\n",
    "    for i in range(0, N_of_file):\n",
    "            DATA[i] = {}\n",
    "\n",
    "    \n",
    "    #Find the minimun length among all the intensity curves\n",
    "    Int_curve_lengths = np.zeros(N_of_file)\n",
    "    for i in range(0, N_of_file):\n",
    "        Int_curve_lengths [i] = (len(Load_Data[i]['RAWDATA'][0,0]['files'][0,0][0]['intensity'][0][0]))\n",
    "    Int_curve_len = (min(Int_curve_lengths.astype(int)))\n",
    "\n",
    "\n",
    "    #### Fill the dictionary ###\n",
    "    Ncell_classified = 200 #Number of classified cells per dataset\n",
    "    for i in range(0, N_of_file):\n",
    "        #Decision array for each dataset\n",
    "        DATA[i]['decision'] = Load_Data[i]['decisionlist']   \n",
    "        #Size of the matrix to store intensity values\n",
    "        NumofData = len(Load_Data[i]['RAWDATA'][0,0]['files'][0,0][0]['intensity'])\n",
    "        #Empty matrix to store all the intensity values of all the cells\n",
    "        DATA[i]['intensity'] = np.zeros((Ncell_classified, Int_curve_len))    \n",
    "        for k in range(0, Ncell_classified): #Only the first 200 were classified\n",
    "            DATA[i]['intensity'][k,:] = Load_Data[i]['RAWDATA'][0,0]['files'][0,0][0]['intensity'][k][0,0:900] #copy the first 900 values\n",
    "\n",
    "\n",
    "    ### Classes array: contain values from all the data ###\n",
    "    Classes=[]\n",
    "    for i in range( 0, N_of_file ):\n",
    "        Classes = np.append(Classes , DATA[i]['decision']).astype(int)\n",
    "\n",
    "\n",
    "    ### Create matrix to store intensity values coming from all the data ###    \n",
    "    NTot_Data = N_of_file*Ncell_classified\n",
    "    Intensity = np.zeros((0, DATA[0]['intensity'].shape[1]))\n",
    "    for i in range( 0, N_of_file ):\n",
    "        Intensity = np.append(Intensity, DATA[i]['intensity'][:,:], axis=0)    \n",
    "\n",
    "    ### ELIMINATE intensity profiles with NaN values and corresponding classes ###\n",
    "    nan_indices = np.where(np.isnan(Intensity))\n",
    "    Intensity = np.delete(Intensity,nan_indices[0], axis = 0)\n",
    "    Classes = np.delete(Classes,nan_indices[0], axis = 0)\n",
    "    print('Intensity curve shape =', Intensity.shape)\n",
    "    print('Classes shape =', Classes.shape)\n",
    "    #Redefine the total number of data\n",
    "    total = len(Intensity)\n",
    "\n",
    "    return Intensity, Classes, Int_curve_len, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DUPLICATE Intensity profiles adding noise to the second group\n",
    "def duplicate_data(total, Intensity, Classes, Int_curve_len):\n",
    "\n",
    "    Intensity_duplicated_noise = np.zeros((2*total, Int_curve_len ))\n",
    "    Intensity_duplicated_noise[0:total,:] = Intensity\n",
    "    Intensity_duplicated_noise[total:2*total,:] = Intensity + np.random.randn(Int_curve_len)*0.02\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.title('Calcium signalling without and with noise added')\n",
    "    plot(Intensity_duplicated_noise[1])\n",
    "    plt.subplot(212)\n",
    "    plot(Intensity_duplicated_noise[991])\n",
    "    plt.xlabel('Frame')\n",
    "    #plt.savefig('13_06_2017_Curve_VS_noiseCurve.png')\n",
    "    plt.show()\n",
    "\n",
    "    #Redefine the total number of data\n",
    "    total = len(Intensity_duplicated_noise)\n",
    "\n",
    "    #DUPLICATE CLASSES\n",
    "    Classes_duplicated = np.append(Classes, Classes)\n",
    "    print('total classes = ', Classes_duplicated.shape)\n",
    "    \n",
    "    return Intensity_duplicated_noise, Classes_duplicated, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform the DOWNSAMPLING of the data: 256 or 128 points\n",
    "def downsample(Intensity, PARAMETERS):\n",
    "    \n",
    "    if PARAMETERS['points_curve'] == 256:   \n",
    "        data1 = Intensity[:,0:768]\n",
    "        step = 3\n",
    "        data_downsample = data1[:,::step]\n",
    "        #Redefine len of the Intensity curves:\n",
    "        Int_profile_len = len(data_downsample[0])        \n",
    "    elif PARAMETERS['points_curve'] == 128:\n",
    "        data1 = Intensity[:,0:896]\n",
    "        step = 7\n",
    "        data_downsample = data1[:,::step]\n",
    "        #Redefine len of the Intensity curves:\n",
    "        Int_profile_len = len(data_downsample[0])    \n",
    "  \n",
    "    \n",
    "    return data_downsample, Int_profile_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_CNN(Data, Classes, total, Name_file, PARAMETERS):\n",
    "    \n",
    "    #Matrix to store test_cl and test_dt  \n",
    "    n_testing_data = total - PARAMETERS['n_training_data']\n",
    "\n",
    "    test_dt = np.zeros(((PARAMETERS['n_iteration'] , n_testing_data, PARAMETERS['points_curve'] )))  #180 data, 256 points for each data, 5 iterations\n",
    "    test_cl = np.zeros(( PARAMETERS['n_iteration'], n_testing_data ))\n",
    "    \n",
    "    #Dictionary to store the restults after each iteration\n",
    "    RESULTS = {} \n",
    "    for iteration in range(PARAMETERS['n_iteration']):\n",
    "        RESULTS[iteration] = {} #dictionary to store the results from the CNN\n",
    "    \n",
    "    #Dictionary to store the predictions (out)\n",
    "    out = {} \n",
    "    for iteration in range(PARAMETERS['n_iteration']):\n",
    "        out[iteration] = {} #dictionary to store the results from the CNN\n",
    "\n",
    "    #Run the CNN for a certain number of iterations\n",
    "    for iteration in range(PARAMETERS['n_iteration']):\n",
    "        print('Iteration: ', iteration)\n",
    "        \n",
    "        ################### Distribute all values randomly ##################   \n",
    "        randseq = np.random.choice(np.arange(0,total),total,replace=False)\n",
    "        \n",
    "        #Data randomly organized\n",
    "        data_random = np.copy(Data)  \n",
    "        data_random = data_random #[randseq,:]\n",
    "        \n",
    "        #Respective Classes \n",
    "        Classes_random = np.copy(Classes)\n",
    "        Classes_random = Classes_random #[randseq]   \n",
    "        \n",
    "        #EXAMPLE\n",
    "        # Data - inputs, 10000 samples of 128-dimensional vectors\n",
    "        # Classes - labels, 10000 samples of scalars from the set {0, 1, 2}\n",
    "\n",
    "        # process the data to fit in a keras CNN properly\n",
    "        # input data needs to be (N, C, X, Y) - shaped where\n",
    "        # N - number of samples\n",
    "        # C - number of channels per sample\n",
    "        # (X, Y) - sample size\n",
    "\n",
    "        #train_dt_reshape = train_dt_reshape.reshape((10000, 128,1, 1))\n",
    "        # output labels should be one-hot vectors - ie,\n",
    "        # 0 -> [1, 0, 0]\n",
    "        # 1 -> [0, 1, 0]\n",
    "        # 2 -> [0, 0, 1]\n",
    "        # this operation changes the shape of y from (10000,1) to (10000, 3)       \n",
    "        \n",
    "        \n",
    "        ######################### load file - TRAINING data ####################\n",
    "        #Data\n",
    "\n",
    "        train_dt = data_random[0:PARAMETERS['n_training_data'],:]\n",
    "        X = np.copy(train_dt)   \n",
    "        #Classes\n",
    "        train_cl = Classes_random[0:PARAMETERS['n_training_data']]        \n",
    "        y = np.copy(train_cl) \n",
    "\n",
    "\n",
    "        # process the data to fit in a keras CNN properly\n",
    "        # input data needs to be (N, C, X, Y) - shaped where\n",
    "        # N - number of samples\n",
    "        # C - number of channels per sample\n",
    "        # (X, Y) - sample size\n",
    "        X = X.reshape((PARAMETERS['n_training_data'], Int_profile_len,1, 1))\n",
    "        y = np_utils.to_categorical(y)\n",
    "\n",
    "        ######################### load file - TESTING data ####################\n",
    "\n",
    "        #Data\n",
    "        test_dt[iteration, :, :]= data_random[PARAMETERS['n_training_data']:total,:]        \n",
    "        test_dt_reshape = np.copy(test_dt[iteration,:,:])\n",
    "\n",
    "\n",
    "        #Classes\n",
    "        test_cl[iteration,:] = Classes_random[PARAMETERS['n_training_data']:total] \n",
    "        test_cl_reshape = np.copy(test_cl[iteration,:])\n",
    "\n",
    "        \n",
    "        # process the data to fit in a keras CNN properly\n",
    "        # input data needs to be (N, C, X, Y) - shaped where\n",
    "        # N - number of samples\n",
    "        # C - number of channels per sample\n",
    "        # (X, Y) - sample size    \n",
    "        test_dt_reshape = test_dt_reshape.reshape((n_testing_data, Int_profile_len,1, 1)) \n",
    "        test_cl_reshape = np_utils.to_categorical(test_cl_reshape)\n",
    "\n",
    "        \n",
    "        #RUN THE CNN network\n",
    "        for i in range(0,len(PARAMETERS['decay_value'])):\n",
    "            print('decay value = ', PARAMETERS['decay_value'][i])\n",
    "            RESULTS[iteration][i], out[iteration][i] = CNN(X, y, test_dt_reshape, test_cl_reshape, n_testing_data, Int_profile_len, Name_file, PARAMETERS, i, iteration)\n",
    "\n",
    "\n",
    "    return RESULTS, out, test_dt, test_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a CNN\n",
    "def CNN(X, y, test_dt_reshape, test_cl_reshape, n_testing_data, Int_profile_len, Name_file, PARAMETERS, i, iteration):\n",
    "\n",
    "    \n",
    "    cnn = Sequential()\n",
    "    cnn.add(Convolution2D(64, 3, 1,border_mode=\"same\",activation=\"relu\",input_shape=(Int_profile_len,1, 1)))\n",
    "    cnn.add(Convolution2D(64, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "\n",
    "    cnn.add(Convolution2D(128, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(Convolution2D(128, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(Convolution2D(128, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "\n",
    "    cnn.add(Convolution2D(256, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(Convolution2D(256, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(Convolution2D(256, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(1024, activation=\"relu\"))\n",
    "    cnn.add(Dropout(PARAMETERS['dropout'])) #0.5\n",
    "    cnn.add(Dense(PARAMETERS['n_classes'], activation=\"softmax\"))\n",
    "\n",
    "    \n",
    "    # define optimizer and objective, COMPILE cnn\n",
    "    compile_step = cnn.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=PARAMETERS['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=PARAMETERS['decay_value'][i]),  metrics=['accuracy'])\n",
    "    #default parameters coming from the paper: https://arxiv.org/abs/1412.6980v8\n",
    "\n",
    "    #Store in dictionaries\n",
    "    results = {}\n",
    "    out = {}  \n",
    "    \n",
    "    #### TRAIN ####\n",
    "    #_DropOut05_data_weights_1980Data_best_DecayValue_\n",
    "    print('decay val CNN', PARAMETERS['decay_value'][i])\n",
    "    filepath =\"/scratch/dwaithe/\" + str(Name_file)+ \"DecValue_\" + str(PARAMETERS['decay_value'][i]) + \"NIteration_\" + str(iteration) + \".hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "     \n",
    "    \n",
    "    # FIT the model\n",
    "    #results = cnn.fit(X, y, batch_size=PARAMETERS['batch_size'], nb_epoch=PARAMETERS['n_epoch'], verbose=0, callbacks=callbacks_list, validation_data=(test_dt_reshape , test_cl_reshape)) \n",
    "    results = cnn.fit(X, y, validation_split=0.33, nb_epoch=PARAMETERS['n_epoch'], batch_size=PARAMETERS['batch_size'], callbacks=callbacks_list, verbose=0)\n",
    " \n",
    "    \n",
    "    \n",
    "    #EVALUATION code which makes prediction\n",
    "    out = cnn.predict(test_dt_reshape.reshape((n_testing_data, Int_profile_len,1, 1)))    \n",
    "    cnn.save('/scratch/dwaithe/'+str(Name_file)+'.h5')\n",
    "   \n",
    "\n",
    "    # save THE MODEL STRUCTURE as YAML file\n",
    "    model_yaml = cnn.to_yaml()\n",
    "    with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)    \n",
    "\n",
    "    \n",
    "    return results, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save LOSS and ACCURACY curve in txt files\n",
    "def Save(RESULTS, PARAMETERS, Name_file):\n",
    "    \n",
    "    #SAVE LOSS    \n",
    "    for iteration in range(PARAMETERS['n_iteration']):\n",
    "        for i in range(len(PARAMETERS['decay_value'])):\n",
    "            np.savetxt(\"/scratch/dwaithe/\"+ str(Name_file)+ \"LOSS_Iter_\"+ str(iteration)+ \"_DecVal_\" +str(PARAMETERS['decay_value'][i])+ \".txt\", RESULTS[iteration][i].history['loss'], newline='\\r\\n')\n",
    "\n",
    "    #SAVE ACCURACY\n",
    "    for iteration in range(PARAMETERS['n_iteration']):\n",
    "        for i in range(len(PARAMETERS['decay_value'])):\n",
    "            np.savetxt(\"/scratch/dwaithe/\" + str(Name_file)+ \"ACC_Iter_\" + str(iteration)+ \"_DecVal_\" +str(PARAMETERS['decay_value'][i]) + \".txt\", RESULTS[iteration][i].history['acc'], newline='\\r\\n')\n",
    "\n",
    "    #SAVE VAL_LOSS\n",
    "    for iteration in range(PARAMETERS['n_iteration']):\n",
    "        for i in range(len(PARAMETERS['decay_value'])):\n",
    "            np.savetxt(\"/scratch/dwaithe/\" + str(Name_file)+ \"VAL_LOSS_Iter_\"+ str(iteration)+ \"_DecVal_\"+str(PARAMETERS['decay_value'][i]) + \".txt\", RESULTS[iteration][i].history['val_loss'], newline='\\r\\n')\n",
    "\n",
    "    #SAVE VAL_ACC\n",
    "    for iteration in range(PARAMETERS['n_iteration']):\n",
    "        for i in range(len(PARAMETERS['decay_value'])):\n",
    "            np.savetxt(\"/scratch/dwaithe/\" + str(Name_file)+ \"VAL_ACC_Iter_\" + str(iteration)+ \"_DecVal_\"+str(PARAMETERS['decay_value'][i]) + \".txt\", RESULTS[iteration][i].history['val_acc'], newline='\\r\\n')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################  MAIN  ###################################\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "PARAMETERS = CNN_parameters()\n",
    "\n",
    "#Choose names for files\n",
    "file_format = input(\"Enter file format: pickle=0 or mat=1. Format:\")\n",
    "Name_file = input('Insert name of the file for the weights between \" \" : ')\n",
    "\n",
    "\n",
    "### Reading Pickle data or .mat data type\n",
    "\n",
    "################################## PICKLE DATA ########################################\n",
    "### lOAD THE DATA STORING THEM IN A DICTIONARY\n",
    "if file_format == 0:\n",
    "    for file in os.listdir( dataDir ):\n",
    "        to_export = pickle.load( open( dataDir+file , \"rb\" ))\n",
    "\n",
    "    Data = to_export['data'] \n",
    "    Classes = to_export['classes']\n",
    "    total = len(Data)\n",
    "    Int_profile_len = len(Data[0])   \n",
    "      \n",
    "    #Run the CNN\n",
    "    RESULTS, test_dt, test_cl = run_CNN(Data, Classes, total, Name_file, PARAMETERS)\n",
    "\n",
    "    \n",
    "    #Save Loss, Validation Loss, Accuracy, Validation Accuracy\n",
    "    Save(RESULTS, n_iteration, Name_file)    \n",
    "\n",
    "############################### .MAT DATA ########################################\n",
    "elif file_format == 1:    # BE CAREFUL: in spyder3 is '1'\n",
    "    \n",
    "    \n",
    "    ### lOAD THE DATA STORING THEM IN A DICTIONARY ###\n",
    "    N_of_file =len(os.listdir( dataDir ))\n",
    "    Load_Data = {}  \n",
    "    for i in range(0, N_of_file):  \n",
    "        Load_Data[i] = {}  #Dictionary for a single workspace\n",
    "    \n",
    "    i = 0\n",
    "    for file in os.listdir( dataDir ):\n",
    "        Load_Data[i] =  scipy.io.loadmat( dataDir+file )\n",
    "        i = i+1    \n",
    "    \n",
    "    #### Define another dictionary for storing intensity curves and classes ### \n",
    "    Intensity, Classes, Int_curve_len, total = data_dict(Load_Data)\n",
    "    \n",
    "    ### Duplicate data adding noise ### \n",
    "    answer = input('Do you want to duplicate the curves adding noise? Yes = 1; No = 0. Answer: ')\n",
    "    if answer == 1:\n",
    "        Intensity, Classes, total = duplicate_data(total, Intensity, Classes, Int_curve_len)\n",
    "        n_training_data = PARAMETERS['n_training_data']        \n",
    "    elif answer == 0:\n",
    "        n_training_data = PARAMETERS['n_training_data'] #input('Number of data to use for training the CNN network: ') #800\n",
    "       \n",
    "    ### Downsampling\n",
    "    data_downsample, Int_profile_len = downsample(Intensity, PARAMETERS)\n",
    "    #fig, ax = plt.subplots()\n",
    "    #ax.plot(data_downsample[16])\n",
    "    \n",
    "    #Check if there are typo errors for the classes (we only have 6 classes going from 0 to 5)\n",
    "    for i in range(len(Classes)):\n",
    "        if Classes[i] > 5:\n",
    "            print('more classes', i)\n",
    "            \n",
    "    \n",
    "    #Run the CNN\n",
    "    RESULTS, out, test_dt, test_cl = run_CNN(data_downsample, Classes, total, Name_file, PARAMETERS)\n",
    "                            \n",
    "    #Save Loss, Validation Loss, Accuracy, Validation Accuracy \n",
    "    Save(RESULTS, PARAMETERS, Name_file)\n",
    "    \n",
    "    end = time.time()\n",
    "    print('Time = ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOSS ={}\n",
    "VAL_LOSS = {}\n",
    "for k in range(len(PARAMETERS['decay_value'])):\n",
    "    LOSS[k] = np.zeros((PARAMETERS['n_iteration'], PARAMETERS['n_epoch'])) #MODIFY THE 20 TO BE WITH VARIABLE\n",
    "    VAL_LOSS[k] = np.zeros((PARAMETERS['n_iteration'], PARAMETERS['n_epoch']))\n",
    "\n",
    "loss_mean = np.zeros((len(PARAMETERS['decay_value']), PARAMETERS['n_epoch'] )) #rows = n. of epochs; columns = n. of decay values\n",
    "val_loss_mean = np.zeros((len(PARAMETERS['decay_value']), PARAMETERS['n_epoch'] ))\n",
    "\n",
    "\n",
    "for k in range(len(PARAMETERS['decay_value'])):   #scan along all the decay values\n",
    "    for iteration in range(PARAMETERS['n_iteration']): #scan along all the iterations\n",
    "        LOSS[k][iteration,:] = np.loadtxt(\"/scratch/dwaithe/\"+ str(Name_file)+ \"LOSS_Iter_\"+ str(iteration)+ \"_DecVal_\" + str(PARAMETERS['decay_value'][k])+ \".txt\")\n",
    "        VAL_LOSS[k][iteration,:] = np.loadtxt(\"/scratch/dwaithe/\"+ str(Name_file)+ \"VAL_LOSS_Iter_\"+ str(iteration)+ \"_DecVal_\" + str(PARAMETERS['decay_value'][k])+ \".txt\")\n",
    "        \n",
    "        #AVERAGE OF THE loss\n",
    "        loss_mean[k,:] = np.mean(LOSS[k], axis = 0) \n",
    "        #AVERAGE OF THE VAL_Loss\n",
    "        val_loss_mean[k,:] = np.mean(VAL_LOSS[k], axis = 0) \n",
    "         \n",
    "\n",
    "#PLOT\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(PARAMETERS['decay_value'])):\n",
    "    ax.plot(LOSS_MEAN[i,:],'-', linewidth=2, label = 'Loss') #+ 'Decay' + str(PARAMETERS['decay_value'][i])) \n",
    "    ax.plot(VAL_LOSS_MEAN[i,:],'o', ms=1, label = 'val_loss') #+ 'Decay' + str(PARAMETERS['decay_value'][i])) \n",
    "\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_title('Average LOSS and VAL_LOSS')\n",
    "legend = plt.legend(loc='upper right', shadow=False, prop={'size':10})\n",
    "fig.savefig('/scratch/dwaithe/' + str(Name_file)+ 'LOSS_and_VAL_LOSS.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot ACC and val_acc - all decay\n",
    "ACC = {}\n",
    "VAL_ACC = {}\n",
    "for k in range(len(PARAMETERS['decay_value'])):\n",
    "    ACC[k] = np.zeros((PARAMETERS['n_iteration'], PARAMETERS['n_epoch'])) #MODIFY THE 20 TO BE WITH VARIABLE\n",
    "    VAL_ACC[k] = np.zeros((PARAMETERS['n_iteration'], PARAMETERS['n_epoch']))\n",
    "\n",
    "acc_mean = np.zeros((len(PARAMETERS['decay_value']), PARAMETERS['n_epoch'] )) #rows = n. of epochs; columns = n. of decay values\n",
    "val_acc_mean = np.zeros((len(PARAMETERS['decay_value']), PARAMETERS['n_epoch'] ))\n",
    "\n",
    "\n",
    "for k in range(len(PARAMETERS['decay_value'])):   #scan along all the decay values\n",
    "    for iteration in range(PARAMETERS['n_iteration']): #scan along all the iterations\n",
    "        ACC[k][iteration,:] = np.loadtxt(\"/scratch/dwaithe/\"+ str(Name_file)+ \"ACC_Iter_\"+ str(iteration)+ \"_DecVal_\" + str(PARAMETERS['decay_value'][k])+ \".txt\")\n",
    "        VAL_ACC[k][iteration,:] = np.loadtxt(\"/scratch/dwaithe/\"+ str(Name_file)+ \"VAL_ACC_Iter_\"+ str(iteration)+ \"_DecVal_\" + str(PARAMETERS['decay_value'][k])+ \".txt\")\n",
    "        \n",
    "        #AVERAGE OF THE ACC\n",
    "        acc_mean[k,:] = np.mean(ACC[k], axis = 0) \n",
    "        #AVERAGE OF THE VAL_ACC\n",
    "        val_acc_mean[k,:] = np.mean(VAL_ACC[k], axis = 0) \n",
    "         \n",
    "\n",
    "#PLOT\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(PARAMETERS['decay_value'])):\n",
    "    ax.plot(acc_mean[i,:],'-', linewidth=2, label = 'Acc'+'Decay001') #)+str(decay_val[i]))\n",
    "    ax.plot(val_acc_mean[i,:],'o', ms=1, label = 'val_Acc'+'Decay001') #+str(decay_val[i]))\n",
    "\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_title('Average ACC and VAL_ACC')\n",
    "legend = plt.legend(loc='upper right', shadow=False, prop={'size':10})\n",
    "fig.savefig('/scratch/dwaithe/' + str(Name_file)+ 'ACC_and_VAL_ACC.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percentage = np.zeros(1*5)\n",
    "k=0\n",
    "for i in range(len(PARAMETERS['decay_value'])):\n",
    "    for iteration in range(PARAMETERS['n_iteration']):\n",
    "    #print('Iteration = ', iteration)\n",
    "        #print('Decay value =', decay_val[i])\n",
    "        percentage[k] = float(np.sum(np.argmax(out[iteration][i],1)==test_cl[iteration]))/float(test_cl[iteration].shape[0])*100.0\n",
    "        #print('Prediction percentage', percentage)\n",
    "        k=k+1\n",
    "print(percentage)\n",
    "np.savetxt(\"/scratch/dwaithe/\"+ str(Name_file)+ \"_Prediction_percentage_NEW.txt\", percentage,  fmt='%.4e', newline='\\r\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create plots with pre-defined labels.\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(15,18):\n",
    "    if test_cl[0,i] == 1:\n",
    "        ax.plot(test_dt[i,:,0], label='Trig+Sing')\n",
    "    if test_cl[0,i] == 2:\n",
    "        ax.plot(test_dt[i,:,0], label='Trig+Osc')\n",
    "    if test_cl[0,i] == 3:\n",
    "        ax.plot(test_dt[i,:,0], label='Trig+Sust')\n",
    "    if test_cl[0,i] == 4:\n",
    "        ax.plot(test_dt[i,:,0], label='No-Trig')\n",
    "    if test_cl[0,i] == 5:\n",
    "        ax.plot(test_dt[i,:,0], label='Not Sure')\n",
    "    legend = plt.legend(loc='upper right', shadow=True, fontsize='x-large')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
